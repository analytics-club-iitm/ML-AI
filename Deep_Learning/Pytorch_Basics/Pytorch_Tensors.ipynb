{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tensors in Pytorch\n",
    "\n",
    "Pytorch is one of the fastest growing libraries in terms of its usage for Tensor Operations and for training Deep Learning models. Through this notebook you'll begin to understand how Pytorch works and how to use it.\n",
    "\n",
    "As we've said earlier, focus more on getting a grasp on the concepts and searching for the syntax on the go. If you don't understand something right away, it's completely fine. Take your time to understand this as it forms a base for everything that's Deep Learning, going forward. Feel free to treat this like a code along and try out everything for yourself!!\n",
    "\n",
    "Happy Learning!!\n",
    "\n",
    "\n",
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Tensors?\n",
    "\n",
    "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other specialized hardware like TPUs to accelerate computing. Therefore, if you have an idea of how NumPy's ndarrays work, you wont have much trouble going through this tutorial.\n",
    "\n",
    "## Initializing a Tensor in Pytorch\n",
    "There are multiple ways to initialise a tensor in pytorch. It can be done randomly or with constant values, directly from a list, through a numpy array or even through another tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Initialised Tensor:  tensor([[0.6753, 0.6810],\n",
      "        [0.1789, 0.6921]])\n",
      "Zero Tensor:  tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "Ones Tensor:  tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "## Random initialisation\n",
    "Random_Tensor = torch.rand(2,2)               # Syntax: torch.rand(shape)\n",
    "print(\"Random Initialised Tensor: \", Random_Tensor)\n",
    "\n",
    "## Constant Initialisation\n",
    "Zeros_Tensor = torch.zeros(2,2)               # Syntax: torch.zeros(shape)\n",
    "print(\"Zero Tensor: \", Zeros_Tensor)\n",
    "\n",
    "Ones_Tensor = torch.ones(2,2)                 # Syntax: torch.ones(shape)\n",
    "print(\"Ones Tensor: \", Ones_Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from a List:  tensor([[2, 3],\n",
      "        [4, 5]])\n",
      "Tensor from a Numpy Array:  tensor([[2, 3],\n",
      "        [4, 5]])\n",
      "Derived from another Tensor:  tensor([[1, 1],\n",
      "        [1, 1]])\n"
     ]
    }
   ],
   "source": [
    "## From a list\n",
    "data = [[2,3], [4,5]]\n",
    "list_tensor = torch.tensor(data)              # Syntax: torch.tensor(list)\n",
    "print(\"Tensor from a List: \", list_tensor)\n",
    "\n",
    "## From a Numpy Array\n",
    "arr = np.array(data)\n",
    "from_numpy_tensor = torch.from_numpy(arr)     # Syntax: torch.from_numpy(numpy-array)\n",
    "print(\"Tensor from a Numpy Array: \", from_numpy_tensor)\n",
    "\n",
    "## From another tensor\n",
    "## The new tensor retains the properties (shape and datatype) of the tensor from which it derives its values\n",
    "Tensor_to_Tensor = torch.ones_like(list_tensor) # Syntax: torch.ones_like(tensor) (can be random initialised too)\n",
    "print(\"Derived from another Tensor: \", Tensor_to_Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Attributes\n",
    "Checking the shape and datatype of a tensor along with the device on which it is stored are at many times an important part of debugging and understanding what your code is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the tensor:  torch.Size([2, 3, 1])\n",
      "Datatype of the tensor:  torch.float32\n",
      "Device on which the tensor is stored:  cpu\n"
     ]
    }
   ],
   "source": [
    "arr = [[[23], [67], [34]], [[45], [67], [89]]]\n",
    "\n",
    "tensor = torch.Tensor(arr)\n",
    "\n",
    "print(\"Size of the tensor: \", tensor.size())  ## Outputs in the form (height, rows, columns)\n",
    "print(\"Datatype of the tensor: \", tensor.dtype)\n",
    "print(\"Device on which the tensor is stored: \", tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Operations\n",
    "Get ready for the real part of this notebook :P\n",
    "\n",
    "Over 100 tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random sampling, and more are possible using pytorch. The major ones and the interesting ones have been included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Moving the Tensor to a GPU if it's available\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "## Since I don't have a GPU on my system, it won't do anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unaltered Tensor:  tensor([[0.3227, 0.8672, 0.5551, 0.2578],\n",
      "        [0.7466, 0.4107, 0.3038, 0.5037],\n",
      "        [0.0211, 0.2466, 0.7796, 0.6068],\n",
      "        [0.6362, 0.2701, 0.2054, 0.0237]])\n",
      "Second column of the Tensor:  tensor([0.8672, 0.4107, 0.2466, 0.2701])\n",
      "Changed Tensor:  tensor([[0.3227, 1.0000, 0.5551, 0.2578],\n",
      "        [0.7466, 1.0000, 0.3038, 0.5037],\n",
      "        [0.0211, 1.0000, 0.7796, 0.6068],\n",
      "        [0.6362, 1.0000, 0.2054, 0.0237]])\n",
      "Second column of the Tensor:  tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "## Indexing and slicing of tensors are just like numpy arrays\n",
    "\n",
    "tensor = torch.rand(4,4)\n",
    "print(\"Unaltered Tensor: \", tensor)\n",
    "print(\"Second column of the Tensor: \", tensor[:, 1])\n",
    "\n",
    "## Let's assign the second column to 1 and see the results again\n",
    "\n",
    "tensor[:,1] = 1\n",
    "print(\"Changed Tensor: \", tensor)\n",
    "print(\"Second column of the Tensor: \", tensor[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:  tensor([[0.5783, 0.1755, 0.2225, 0.6179],\n",
      "        [0.7338, 0.9973, 0.7760, 0.3432],\n",
      "        [0.5797, 0.6951, 0.0669, 0.0626],\n",
      "        [0.5835, 0.0276, 0.7835, 0.7523]])\n",
      " Original Tensor Size:  torch.Size([4, 4])\n",
      "Tensors concatenated along their column:  tensor([[0.5783, 0.1755, 0.2225, 0.6179, 0.5783, 0.1755, 0.2225, 0.6179, 0.5783,\n",
      "         0.1755, 0.2225, 0.6179],\n",
      "        [0.7338, 0.9973, 0.7760, 0.3432, 0.7338, 0.9973, 0.7760, 0.3432, 0.7338,\n",
      "         0.9973, 0.7760, 0.3432],\n",
      "        [0.5797, 0.6951, 0.0669, 0.0626, 0.5797, 0.6951, 0.0669, 0.0626, 0.5797,\n",
      "         0.6951, 0.0669, 0.0626],\n",
      "        [0.5835, 0.0276, 0.7835, 0.7523, 0.5835, 0.0276, 0.7835, 0.7523, 0.5835,\n",
      "         0.0276, 0.7835, 0.7523]])\n",
      "Column Concatenated Tensor Size:  torch.Size([4, 12])\n",
      "Tensors concatenated along their row:  tensor([[0.5783, 0.1755, 0.2225, 0.6179],\n",
      "        [0.7338, 0.9973, 0.7760, 0.3432],\n",
      "        [0.5797, 0.6951, 0.0669, 0.0626],\n",
      "        [0.5835, 0.0276, 0.7835, 0.7523],\n",
      "        [0.5783, 0.1755, 0.2225, 0.6179],\n",
      "        [0.7338, 0.9973, 0.7760, 0.3432],\n",
      "        [0.5797, 0.6951, 0.0669, 0.0626],\n",
      "        [0.5835, 0.0276, 0.7835, 0.7523],\n",
      "        [0.5783, 0.1755, 0.2225, 0.6179],\n",
      "        [0.7338, 0.9973, 0.7760, 0.3432],\n",
      "        [0.5797, 0.6951, 0.0669, 0.0626],\n",
      "        [0.5835, 0.0276, 0.7835, 0.7523]])\n",
      "Row Concatenated Tensor Size:  torch.Size([12, 4])\n"
     ]
    }
   ],
   "source": [
    "## Concatenating a sequence of tensors along a given dimension\n",
    "\n",
    "tensor = torch.rand(4,4)\n",
    "print(\"Original Tensor: \", tensor)\n",
    "print(\" Original Tensor Size: \", tensor.size())\n",
    "\n",
    "## Along columns\n",
    "column_cat_tensor = torch.cat([tensor, tensor, tensor], dim = 1)\n",
    "print(\"Tensors concatenated along their column: \", column_cat_tensor)\n",
    "print(\"Column Concatenated Tensor Size: \", column_cat_tensor.size())\n",
    "\n",
    "## Along Rows\n",
    "row_cat_tensor = torch.cat([tensor, tensor, tensor], dim = 0)\n",
    "print(\"Tensors concatenated along their row: \", row_cat_tensor)\n",
    "print(\"Row Concatenated Tensor Size: \", row_cat_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative? Or not?\n",
    "torch.stack() is another function that can help you acheive similar functionality.\n",
    "\n",
    "The difference between torch.stack() and torch.cat() is that torch.cat() will join all the tensors along the dimension that you specify but torch.stack() will always join the tensors along a new dimension. Therefore if you have three (2,2) tensors, after stacking them, you'll have the new tensors shape as (3,2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element wise addition of tensors:  tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "Element wise addition using add:  tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "Element wise subtraction of tensors:  tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "Element wise subtraction using subtract:  tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "Element wise multiplied tensors:  tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "Matrix Multiplied Tensor:  tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "Element wise divided Tensors tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.Tensor([[1,2], [3,4]])\n",
    "tensor2 = torch.Tensor([[5,6], [7,8]])\n",
    "\n",
    "## Element wise addition of the tensors\n",
    "print(\"Element wise addition of tensors: \", tensor1 + tensor2)\n",
    "## Can also be done with the add function\n",
    "print(\"Element wise addition using add: \", tensor1.add(tensor2))\n",
    "\n",
    "## Element wise subtraction\n",
    "print(\"Element wise subtraction of tensors: \", tensor1 - tensor2)\n",
    "## Can also be done with the subtract function\n",
    "print(\"Element wise subtraction using subtract: \", tensor1.subtract(tensor2))\n",
    "\n",
    "## Element wise multiplication of tensors\n",
    "\n",
    "elem_mul = tensor1*tensor2\n",
    "print(\"Element wise multiplied tensors: \", elem_mul)\n",
    "\n",
    "## Matrix Multiplication of the tensors\n",
    "\n",
    "mat_mul = tensor1.matmul(tensor2)\n",
    "print(\"Matrix Multiplied Tensor: \", mat_mul)\n",
    "\n",
    "## Element wise division of integers\n",
    "print(\"Element wise divided Tensors\", tensor1/tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Place Operations\n",
    "In place operations are those operations where in writing, one does not need to reassign the value to the given variable. The backend takes care of it on its own.\n",
    "\n",
    "In Pytorch this is done by adding an underscore after an operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:  tensor([[0.9956, 0.1688],\n",
      "        [0.1237, 0.2043]])\n",
      "Tensor with one added to all it's elements:  tensor([[1.9956, 1.1688],\n",
      "        [1.1237, 1.2043]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(2,2)\n",
    "print(\"Original tensor: \", tensor)\n",
    "\n",
    "tensor.add_(1)       ## No reassigning required\n",
    "print(\"Tensor with one added to all it's elements: \", tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A hint of caution!!\n",
    "Tensors and Numpy arrays share their location on a CPU and changing one will lead to a change in the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:  tensor([2., 3., 4., 5., 6., 7.])\n",
      "Numpy Array:  [2. 3. 4. 5. 6. 7.]\n",
      "Tensor to which we added 2:  tensor([4., 5., 6., 7., 8., 9.])\n",
      "Numpy array to which we apparently did nothing:  [4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.Tensor([2,3,4,5,6,7])\n",
    "\n",
    "arr = tensor.numpy()\n",
    "\n",
    "print(\"Tensor: \", tensor)\n",
    "print(\"Numpy Array: \", arr)\n",
    "\n",
    "tensor.add_(2)\n",
    "\n",
    "print(\"Tensor to which we added 2: \", tensor)\n",
    "print(\"Numpy array to which we apparently did nothing: \", arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Interesting Functions\n",
    "We now go on to explore some really interesting and useful functionality that pytorch has to offer which really makes our lives easier when we face errors in a code or need to carry out some complex operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk\n",
    "Splits a tensor into a specific number of chunks. Each chunk is a view of the input tensor.\n",
    "\n",
    "Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unaltered Tensor:  tensor([[0.7551, 0.5355, 0.4830, 0.9313],\n",
      "        [0.3397, 0.9135, 0.9729, 0.8798],\n",
      "        [0.2965, 0.6759, 0.6492, 0.8820],\n",
      "        [0.7308, 0.1039, 0.6183, 0.8919]])\n",
      "First Chunk:  tensor([[0.7551, 0.5355],\n",
      "        [0.3397, 0.9135],\n",
      "        [0.2965, 0.6759],\n",
      "        [0.7308, 0.1039]])\n",
      "Second Chunk:  tensor([[0.4830, 0.9313],\n",
      "        [0.9729, 0.8798],\n",
      "        [0.6492, 0.8820],\n",
      "        [0.6183, 0.8919]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(4,4)\n",
    "print(\"Unaltered Tensor: \", tensor)\n",
    "\n",
    "t_list = torch.chunk(tensor, chunks = 2, dim = 1) #Splitting along columns\n",
    "print(\"First Chunk: \", t_list[0])\n",
    "print(\"Second Chunk: \", t_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
