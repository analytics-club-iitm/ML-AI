{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tensors in Pytorch\n",
    "\n",
    "Pytorch is one of the fastest growing libraries in terms of its usage for Tensor Operations and for training Deep Learning models. Through this notebook you'll begin to understand how Pytorch works and how to use it.\n",
    "\n",
    "As we've said earlier, focus more on getting a grasp on the concepts and searching for the syntax on the go. If you don't understand something right away, it's completely fine. Take your time to understand this as it forms a base for everything that's Deep Learning, going forward. Feel free to treat this like a code along and try out everything for yourself!!\n",
    "\n",
    "Happy Learning!!\n",
    "\n",
    "\n",
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:33:52.398213Z",
     "start_time": "2021-01-05T22:33:46.741653Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Tensors?\n",
    "\n",
    "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other specialized hardware like TPUs to accelerate computing. Therefore, if you have an idea of how NumPy's ndarrays work, you wont have much trouble going through this tutorial.\n",
    "\n",
    "## Initializing a Tensor in Pytorch\n",
    "There are multiple ways to initialise a tensor in pytorch. It can be done randomly or with constant values, directly from a list, through a numpy array or even through another tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:33:53.067041Z",
     "start_time": "2021-01-05T22:33:52.400006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Initialised Tensor:  tensor([[0.7808, 0.7059],\n",
      "        [0.8180, 0.5520]])\n",
      "Zero Tensor:  tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "Ones Tensor:  tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "## Random initialisation\n",
    "Random_Tensor = torch.rand(2,2)               # Syntax: torch.rand(shape)\n",
    "print(\"Random Initialised Tensor: \", Random_Tensor)\n",
    "\n",
    "## Constant Initialisation\n",
    "Zeros_Tensor = torch.zeros(2,2)               # Syntax: torch.zeros(shape)\n",
    "print(\"Zero Tensor: \", Zeros_Tensor)\n",
    "\n",
    "Ones_Tensor = torch.ones(2,2)                 # Syntax: torch.ones(shape)\n",
    "print(\"Ones Tensor: \", Ones_Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:33:53.077282Z",
     "start_time": "2021-01-05T22:33:53.069676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from a List:  tensor([[2, 3],\n",
      "        [4, 5]])\n",
      "Tensor from a Numpy Array:  tensor([[2, 3],\n",
      "        [4, 5]])\n",
      "Derived from another Tensor:  tensor([[1, 1],\n",
      "        [1, 1]])\n"
     ]
    }
   ],
   "source": [
    "## From a list\n",
    "data = [[2,3], [4,5]]\n",
    "list_tensor = torch.tensor(data)              # Syntax: torch.tensor(list)\n",
    "print(\"Tensor from a List: \", list_tensor)\n",
    "\n",
    "## From a Numpy Array\n",
    "arr = np.array(data)\n",
    "from_numpy_tensor = torch.from_numpy(arr)     # Syntax: torch.from_numpy(numpy-array)\n",
    "print(\"Tensor from a Numpy Array: \", from_numpy_tensor)\n",
    "\n",
    "## From another tensor\n",
    "## The new tensor retains the properties (shape and datatype) of the tensor from which it derives its values\n",
    "Tensor_to_Tensor = torch.ones_like(list_tensor) # Syntax: torch.ones_like(tensor) (can be random initialised too)\n",
    "print(\"Derived from another Tensor: \", Tensor_to_Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Attributes\n",
    "Checking the shape and datatype of a tensor along with the device on which it is stored are at many times an important part of debugging and understanding what your code is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:33:53.138526Z",
     "start_time": "2021-01-05T22:33:53.079572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the tensor:  torch.Size([2, 3, 1])\n",
      "Datatype of the tensor:  torch.float32\n",
      "Device on which the tensor is stored:  cpu\n"
     ]
    }
   ],
   "source": [
    "arr = [[[23], [67], [34]], [[45], [67], [89]]]\n",
    "\n",
    "tensor = torch.Tensor(arr)\n",
    "\n",
    "print(\"Size of the tensor: \", tensor.size())  ## Outputs in the form (height, rows, columns)\n",
    "print(\"Datatype of the tensor: \", tensor.dtype)\n",
    "print(\"Device on which the tensor is stored: \", tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Operations\n",
    "Get ready for the real part of this notebook :P\n",
    "\n",
    "Over 100 tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random sampling, and more are possible using pytorch. The major ones and the interesting ones have been included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:33:53.279020Z",
     "start_time": "2021-01-05T22:33:53.142932Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryan/Documents/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "## Moving the Tensor to a GPU if it's available\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "## Since I don't have a GPU on my system, it won't do anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:33:53.355334Z",
     "start_time": "2021-01-05T22:33:53.281459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unaltered Tensor:  tensor([[0.8210, 0.2034, 0.4684, 0.4551],\n",
      "        [0.4915, 0.7234, 0.7116, 0.8786],\n",
      "        [0.7132, 0.4278, 0.1834, 0.7875],\n",
      "        [0.3702, 0.1139, 0.2231, 0.2272]])\n",
      "Second column of the Tensor:  tensor([0.2034, 0.7234, 0.4278, 0.1139])\n",
      "Changed Tensor:  tensor([[0.8210, 1.0000, 0.4684, 0.4551],\n",
      "        [0.4915, 1.0000, 0.7116, 0.8786],\n",
      "        [0.7132, 1.0000, 0.1834, 0.7875],\n",
      "        [0.3702, 1.0000, 0.2231, 0.2272]])\n",
      "Second column of the Tensor:  tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "## Indexing and slicing of tensors are just like numpy arrays\n",
    "\n",
    "tensor = torch.rand(4,4)\n",
    "print(\"Unaltered Tensor: \", tensor)\n",
    "print(\"Second column of the Tensor: \", tensor[:, 1])\n",
    "\n",
    "## Let's assign the second column to 1 and see the results again\n",
    "\n",
    "tensor[:,1] = 1\n",
    "print(\"Changed Tensor: \", tensor)\n",
    "print(\"Second column of the Tensor: \", tensor[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:33:53.509840Z",
     "start_time": "2021-01-05T22:33:53.359052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:  tensor([[0.8955, 0.2876, 0.5555, 0.2102],\n",
      "        [0.0457, 0.7085, 0.7415, 0.8742],\n",
      "        [0.3727, 0.3809, 0.2354, 0.5723],\n",
      "        [0.0754, 0.3260, 0.3049, 0.3185]])\n",
      " Original Tensor Size:  torch.Size([4, 4])\n",
      "Tensors concatenated along their column:  tensor([[0.8955, 0.2876, 0.5555, 0.2102, 0.8955, 0.2876, 0.5555, 0.2102, 0.8955,\n",
      "         0.2876, 0.5555, 0.2102],\n",
      "        [0.0457, 0.7085, 0.7415, 0.8742, 0.0457, 0.7085, 0.7415, 0.8742, 0.0457,\n",
      "         0.7085, 0.7415, 0.8742],\n",
      "        [0.3727, 0.3809, 0.2354, 0.5723, 0.3727, 0.3809, 0.2354, 0.5723, 0.3727,\n",
      "         0.3809, 0.2354, 0.5723],\n",
      "        [0.0754, 0.3260, 0.3049, 0.3185, 0.0754, 0.3260, 0.3049, 0.3185, 0.0754,\n",
      "         0.3260, 0.3049, 0.3185]])\n",
      "Column Concatenated Tensor Size:  torch.Size([4, 12])\n",
      "Tensors concatenated along their row:  tensor([[0.8955, 0.2876, 0.5555, 0.2102],\n",
      "        [0.0457, 0.7085, 0.7415, 0.8742],\n",
      "        [0.3727, 0.3809, 0.2354, 0.5723],\n",
      "        [0.0754, 0.3260, 0.3049, 0.3185],\n",
      "        [0.8955, 0.2876, 0.5555, 0.2102],\n",
      "        [0.0457, 0.7085, 0.7415, 0.8742],\n",
      "        [0.3727, 0.3809, 0.2354, 0.5723],\n",
      "        [0.0754, 0.3260, 0.3049, 0.3185],\n",
      "        [0.8955, 0.2876, 0.5555, 0.2102],\n",
      "        [0.0457, 0.7085, 0.7415, 0.8742],\n",
      "        [0.3727, 0.3809, 0.2354, 0.5723],\n",
      "        [0.0754, 0.3260, 0.3049, 0.3185]])\n",
      "Row Concatenated Tensor Size:  torch.Size([12, 4])\n"
     ]
    }
   ],
   "source": [
    "## Concatenating a sequence of tensors along a given dimension\n",
    "\n",
    "tensor = torch.rand(4,4)\n",
    "print(\"Original Tensor: \", tensor)\n",
    "print(\" Original Tensor Size: \", tensor.size())\n",
    "\n",
    "## Along columns\n",
    "column_cat_tensor = torch.cat([tensor, tensor, tensor], dim = 1)\n",
    "print(\"Tensors concatenated along their column: \", column_cat_tensor)\n",
    "print(\"Column Concatenated Tensor Size: \", column_cat_tensor.size())\n",
    "\n",
    "## Along Rows\n",
    "row_cat_tensor = torch.cat([tensor, tensor, tensor], dim = 0)\n",
    "print(\"Tensors concatenated along their row: \", row_cat_tensor)\n",
    "print(\"Row Concatenated Tensor Size: \", row_cat_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative? Or not?\n",
    "torch.stack() is another function that can help you acheive similar functionality.\n",
    "\n",
    "The difference between torch.stack() and torch.cat() is that torch.cat() will join all the tensors along the dimension that you specify but torch.stack() will always join the tensors along a new dimension. Therefore if you have three (2,2) tensors, after stacking them, you'll have the new tensors shape as (3,2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:33:53.811263Z",
     "start_time": "2021-01-05T22:33:53.512460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element wise addition of tensors:  tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "Element wise addition using add:  tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "Element wise subtraction of tensors:  tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "Element wise subtraction using subtract:  tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "Element wise multiplied tensors:  tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "Matrix Multiplied Tensor:  tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "Element wise divided Tensors tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.Tensor([[1,2], [3,4]])\n",
    "tensor2 = torch.Tensor([[5,6], [7,8]])\n",
    "\n",
    "## Element wise addition of the tensors\n",
    "print(\"Element wise addition of tensors: \", tensor1 + tensor2)\n",
    "## Can also be done with the add function\n",
    "print(\"Element wise addition using add: \", tensor1.add(tensor2))\n",
    "\n",
    "## Element wise subtraction\n",
    "print(\"Element wise subtraction of tensors: \", tensor1 - tensor2)\n",
    "## Can also be done with the subtract function\n",
    "print(\"Element wise subtraction using subtract: \", tensor1.subtract(tensor2))\n",
    "\n",
    "## Element wise multiplication of tensors\n",
    "\n",
    "elem_mul = tensor1*tensor2\n",
    "print(\"Element wise multiplied tensors: \", elem_mul)\n",
    "\n",
    "## Matrix Multiplication of the tensors\n",
    "\n",
    "mat_mul = tensor1.matmul(tensor2)\n",
    "print(\"Matrix Multiplied Tensor: \", mat_mul)\n",
    "\n",
    "## Element wise division of integers\n",
    "print(\"Element wise divided Tensors\", tensor1/tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Place Operations\n",
    "In place operations are those operations where in writing, one does not need to reassign the value to the given variable. The backend takes care of it on its own.\n",
    "\n",
    "In Pytorch this is done by adding an underscore after an operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:33:53.825155Z",
     "start_time": "2021-01-05T22:33:53.815906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:  tensor([[0.4984, 0.7754],\n",
      "        [0.2567, 0.1503]])\n",
      "Tensor with one added to all it's elements:  tensor([[1.4984, 1.7754],\n",
      "        [1.2567, 1.1503]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(2,2)\n",
    "print(\"Original tensor: \", tensor)\n",
    "\n",
    "tensor.add_(1)       ## No reassigning required\n",
    "print(\"Tensor with one added to all it's elements: \", tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A hint of caution!!\n",
    "Tensors and Numpy arrays share their location on a CPU and changing one will lead to a change in the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:33:53.883957Z",
     "start_time": "2021-01-05T22:33:53.827586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:  tensor([2., 3., 4., 5., 6., 7.])\n",
      "Numpy Array:  [2. 3. 4. 5. 6. 7.]\n",
      "Tensor to which we added 2:  tensor([4., 5., 6., 7., 8., 9.])\n",
      "Numpy array to which we apparently did nothing:  [4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.Tensor([2,3,4,5,6,7])\n",
    "\n",
    "arr = tensor.numpy()\n",
    "\n",
    "print(\"Tensor: \", tensor)\n",
    "print(\"Numpy Array: \", arr)\n",
    "\n",
    "tensor.add_(2)\n",
    "\n",
    "print(\"Tensor to which we added 2: \", tensor)\n",
    "print(\"Numpy array to which we apparently did nothing: \", arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Interesting Functions\n",
    "We now go on to explore some really interesting and useful functionality that pytorch has to offer which really makes our lives easier when we face errors in a code or need to carry out some complex operation. If you don't understand any of these functions, check out the Pytorch documentation for a detailed explanation of these functions or check a stackoverflow answer for the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk\n",
    "Splits a tensor into a specific number of chunks. Each chunk is a view of the input tensor.\n",
    "\n",
    "Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:33:53.973511Z",
     "start_time": "2021-01-05T22:33:53.885248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unaltered Tensor:  tensor([[0.8160, 0.7151, 0.6681, 0.2222],\n",
      "        [0.4219, 0.5489, 0.3397, 0.0842],\n",
      "        [0.0312, 0.3272, 0.6585, 0.1668],\n",
      "        [0.4473, 0.6781, 0.9432, 0.0159]])\n",
      "First Chunk:  tensor([[0.8160, 0.7151],\n",
      "        [0.4219, 0.5489],\n",
      "        [0.0312, 0.3272],\n",
      "        [0.4473, 0.6781]])\n",
      "Second Chunk:  tensor([[0.6681, 0.2222],\n",
      "        [0.3397, 0.0842],\n",
      "        [0.6585, 0.1668],\n",
      "        [0.9432, 0.0159]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(4,4)\n",
    "print(\"Unaltered Tensor: \", tensor)\n",
    "\n",
    "t_list = torch.chunk(tensor, chunks = 2, dim = 1) #Splitting along columns\n",
    "print(\"First Chunk: \", t_list[0])\n",
    "print(\"Second Chunk: \", t_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape\n",
    "\n",
    "Returns a tensor with the same data and number of elements as the input, but with the specified shape. \n",
    "\n",
    "Note that the same number of elements need to be there in both the tensors else an error will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:33:54.018048Z",
     "start_time": "2021-01-05T22:33:53.976281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unaltered Tensor:  tensor([[0.4880, 0.1121, 0.6619, 0.6177],\n",
      "        [0.9546, 0.6231, 0.4393, 0.8199],\n",
      "        [0.4334, 0.3649, 0.2798, 0.6455],\n",
      "        [0.4214, 0.6051, 0.9555, 0.3437]])\n",
      "Reshaped Tensor:  tensor([[0.4880, 0.1121],\n",
      "        [0.6619, 0.6177],\n",
      "        [0.9546, 0.6231],\n",
      "        [0.4393, 0.8199],\n",
      "        [0.4334, 0.3649],\n",
      "        [0.2798, 0.6455],\n",
      "        [0.4214, 0.6051],\n",
      "        [0.9555, 0.3437]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(4,4)\n",
    "print(\"Unaltered Tensor: \", tensor)\n",
    "\n",
    "t_reshaped = torch.reshape(tensor, (8,2))\n",
    "print(\"Reshaped Tensor: \", t_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeeze and Unsqueeze\n",
    "\n",
    "These commands are used to add or remove dimensions in a tensor which have only a single element. Squeeze is responsible for removing the dimensions where there is only a single element present whereas Unsqueeze is resposible for adding an extra dimension where only a single element is present at the specified position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:34:06.609507Z",
     "start_time": "2021-01-05T22:34:06.602606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unaltered Tensor size:  torch.Size([4, 4, 1, 4])\n",
      "Squeezed Tensor size:  torch.Size([4, 4, 4])\n",
      "Unsqueezed Tensor size:  torch.Size([4, 4, 1, 4])\n",
      "Further Unsqueezed Tensor size:  torch.Size([4, 1, 4, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(4,4,1,4)\n",
    "print(\"Unaltered Tensor size: \", tensor.size())\n",
    "\n",
    "t_squeeze = torch.squeeze(tensor)\n",
    "print(\"Squeezed Tensor size: \", t_squeeze.size())\n",
    "\n",
    "t_unsqueeze_1 = torch.unsqueeze(t_squeeze, dim = 2)     #Dim is the position at which to insert a dimension\n",
    "print(\"Unsqueezed Tensor size: \", t_unsqueeze_1.size())\n",
    "\n",
    "t_unsqueeze_2 = torch.unsqueeze(t_unsqueeze_1, dim = 1)\n",
    "print(\"Further Unsqueezed Tensor size: \", t_unsqueeze_2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbind\n",
    "\n",
    "Removes a tensor dimension.\n",
    "\n",
    "Returns a tuple of all slices along a given dimension, already without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:38:24.022504Z",
     "start_time": "2021-01-05T22:38:24.005329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unaltered Tensor:  tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "Tensor unbound along the first dimension:  (tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))\n",
      "Tensor unbound along the second dimension:  (tensor([1, 4, 7]), tensor([2, 5, 8]), tensor([3, 6, 9]))\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(\"Unaltered Tensor: \", tensor)\n",
    "\n",
    "t_unbind_0 = torch.unbind(tensor, dim = 0)\n",
    "print(\"Tensor unbound along the first dimension: \", t_unbind_0)\n",
    "\n",
    "t_unbind_1 = torch.unbind(tensor, dim = 1)\n",
    "print(\"Tensor unbound along the second dimension: \", t_unbind_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where\n",
    "\n",
    "Return a tensor of elements selected from either x or y, depending on condition.\n",
    "\n",
    "The operation is defined as:\n",
    "\n",
    "\\begin{cases} \\text{x}_i & \\text{if } \\text{condition}_i \\\\ \\text{y}_i & \\text{otherwise} \\\\ \\end{cases}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:46:41.769182Z",
     "start_time": "2021-01-05T22:46:41.603617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x:  tensor([[ 0.1361,  0.3214],\n",
      "        [-1.7902,  2.1678],\n",
      "        [-0.4322, -1.7160]])\n",
      "Tensor y:  tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "The Tensor after the Where Operation:  tensor([[0.1361, 0.3214],\n",
      "        [1.0000, 2.1678],\n",
      "        [1.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,2)\n",
    "y = torch.ones(3,2)\n",
    "\n",
    "print(\"Tensor x: \", x)\n",
    "print(\"Tensor y: \", y)\n",
    "\n",
    "t_where = torch.where(x>0, x, y)\n",
    "print(\"The Tensor after the Where Operation: \", t_where)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that at all the places where the value of x was less than zero, the function has picked up the values from y which are all 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Select\n",
    "\n",
    "Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T22:54:42.770355Z",
     "start_time": "2021-01-05T22:54:42.763937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unaltered Tensor:  tensor([[ 1.1746, -0.5701],\n",
      "        [ 1.2900, -0.7144],\n",
      "        [-0.4941, -1.0164]])\n",
      "Mask Tensor:  tensor([[ True, False],\n",
      "        [ True, False],\n",
      "        [False, False]])\n",
      "The selected tensor elements based on the mask:  tensor([1.1746, 1.2900])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(3,2)\n",
    "print(\"Unaltered Tensor: \", tensor)\n",
    "\n",
    "mask = tensor.ge(0.5)    #This command compares all the elements of the tensor with the input value(0.5)\n",
    "print(\"Mask Tensor: \", mask)\n",
    "\n",
    "t_masked = torch.masked_select(tensor, mask)\n",
    "print(\"The selected tensor elements based on the mask: \", t_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather\n",
    "\n",
    "Gathers values along an axis specified by dim.\n",
    "\n",
    "For a 2-D tensor the output is specified by:\n",
    "\n",
    "out [i] [j] = input [index [i] [j]] [j]  # if dim == 0\n",
    "\n",
    "out [i] [j] = input [i] [index [i] [j]]  # if dim == 1\n",
    "\n",
    "If this isn't clear yet, don't worry let's look at it with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T23:01:20.550900Z",
     "start_time": "2021-01-05T23:01:20.399079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unaltered Tensor:  tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Index Tensor:  tensor([[0, 0],\n",
      "        [1, 0]])\n",
      "The gathered Tensor is:  tensor([[1, 1],\n",
      "        [4, 3]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[1,2], [3,4]])\n",
    "index = torch.tensor([[0,0], [1,0]])\n",
    "\n",
    "print(\"Unaltered Tensor: \", tensor)\n",
    "print(\"Index Tensor: \", index)\n",
    "\n",
    "t_gather = torch.gather(tensor, dim = 1, index = index)\n",
    "print(\"The gathered Tensor is: \", t_gather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what had happened here. Going by the definition given in the previous markdown cell, for dim = 1, we see that the output out [i] [j] is given by input [i] [index [i] [j]]. \n",
    "\n",
    "Let us take the case where i = 0 and j = 1. input [i] then contains the row [1,2] from the tensor. index [i] [j] contains the element 0. therefore for the element at the 0,1 position in the output, the value is given by input [0] [0] which is 1.\n",
    "\n",
    "Now, Let us take the case where i = 1 and j = 0. input [i] then contains the row [3,4] from the tensor. index [i] [j] contains the element 1. therefore for the element at the 1,0 position in the output, the value is given by input [1] [1] which is 4.\n",
    "\n",
    "Hope this makes some sense.\n",
    "\n",
    "If it still doesn't make sense, take a look at this stackoverflow answer: [Gather Function in Layman Terms](https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for this tutorial folks!!\n",
    "\n",
    "For further learning and checking out some more cool Pytorch Functions, take a look at the link given to the documentation: [Pytorch Documentation](https://pytorch.org/docs/stable/torch.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
